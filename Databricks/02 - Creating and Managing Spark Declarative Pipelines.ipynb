{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e8182d3-de3e-451e-8efd-2310f98f0670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![databricks_academy_logo.png](../Includes/images/databricks_academy_logo.png \"databricks_academy_logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a27352af-64c4-4eb4-aa62-de3c48143f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating and Managing Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "Lakeflow Spark Declarative Pipelines provides a collection of tools that allow you to orchestrate ETL pipelines with ease on Databricks.\n",
    "\n",
    "**Objective:** Use Databricks Lakeflow Spark Declarative Pipelines to create an ETL pipeline. The pipeline will create three tables, which will be refreshed every time the pipeline runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd14e49f-714c-4c75-aeb7-b1ad76b0a494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Important: Select Environment 4\n",
    "The cells below may not work in other environments. To choose environment 4: \n",
    "1. Click the ![environment.png](../Includes/images/environment.png \"environment.png\") button on the right sidebar\n",
    "1. Open the **Environment version** dropdown\n",
    "1. Select **4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00cf2229-fc31-44a7-8261-2c304d36d6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6694ba9-9290-440b-b5e2-61bf515f6bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK PATHS FOR TASKS:\n\n  * Root folder path: \n   /Users/alladisindhu24@gmail.com/Data Engineering/M-03 -- Pipelines/Pipeline Files\n\n  * Source file path: \n   /Users/alladisindhu24@gmail.com/Data Engineering/M-03 -- Pipelines/Pipeline Files/Pipeline - 1.py\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# Set python variables for catalog, schema, and volume names (change, if desired)\n",
    "catalog_name = \"dbacademy\"\n",
    "schema_name = \"create_pipeline\"\n",
    "volume_name = \"myfiles\"\n",
    "####################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Create the catalog, schema, and volume if they don't exist already\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_name}\")\n",
    "####################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Creates a file called employees.csv in the specified catalog.schema.volume\n",
    "import pandas as pd\n",
    "data = [\n",
    "    [\"1111\", \"Kristi\", \"USA\", \"Manager\"],\n",
    "    [\"2222\", \"Sophia\", \"Greece\", \"Developer\"],\n",
    "    [\"3333\", \"Peter\", \"USA\", \"Developer\"],\n",
    "    [\"4444\", \"Zebi\", \"Pakistan\", \"Administrator\"]\n",
    "]\n",
    "columns = [\"ID\", \"Firstname\", \"Country\", \"Role\"] \n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "file_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/employees.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Creates a file called employees2.csv in the specified catalog.schema.volume\n",
    "data = [\n",
    "    [5555, 'Alex','USA', 'Instructor'],\n",
    "    [6666, 'Sanjay','India', 'Instructor']\n",
    "]\n",
    "columns = [\"ID\",\"Firstname\", \"Country\", \"Role\"]\n",
    "\n",
    "## Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "## Create the CSV file in the course Catalog.Schema.Volume\n",
    "df.to_csv(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/employees2.csv\", index=False)\n",
    "####################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Print paths to root folder and source code file\n",
    "path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "newpath = path.replace('02 - Creating and Managing Spark Declarative Pipelines','Pipeline Files')\n",
    "rootpath = newpath\n",
    "sourcefilepath = newpath + '/Pipeline - 1.py'\n",
    "print(f'NOTEBOOK PATHS FOR TASKS:\\n')\n",
    "print(f'  * Root folder path: \\n   {rootpath}\\n')\n",
    "print(f'  * Source file path: \\n   {sourcefilepath}')\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e0671f4-0bb2-405c-a2a2-a0d0d2f48cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a Pipeline\n",
    "In this lesson we have starter files for you to use in your pipeline. These are located in the folder **Pipeline Files**. To create a pipeline and add existing assets to associate it with code files already available in your workspace, complete the following:\n",
    "\n",
    "1. Right click **Jobs & Pipelines** in the left navigation bar and select **Open Link in New Tab**.\n",
    "\n",
    "2. In **Jobs & Pipelines** select **Create** → **ETL Pipeline**.\n",
    "\n",
    "3. Complete the pipeline creation page with the following: \n",
    "\n",
    "    * **Name**: Name the pipeline whatever you wish\n",
    "    * **Default catalog**: Select the **dbacademy** catalog (or a different one if you changed the default at the beginning of the lesson) \n",
    "    * **Default schema**: Select the **create_pipeline** schema (or a different one if you changed the default at the beginning of the lesson)\n",
    "\n",
    "4. In the options, select **Add existing assets**. In the popup, complete the following:\n",
    "\n",
    "- **Pipeline root folder**: Select the **Pipeline Files** folder (path is in the output of the previous cell) \n",
    "\n",
    "- **Source code paths**: Within the same root folder as above, select the **Pipeline - 1.py** file\n",
    "\n",
    "5. Click **Add**, This will create a pipeline and associate the correct files for this demonstration.\n",
    "\n",
    "  Note: We will discuss the source code for this pipeline in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58427841-0427-4c24-9ecf-94451dcaea35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy a Pipeline\n",
    "Let's look at how to deploy a pipeline to production.\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "  a. Click **Settings** in the upper-right corner (this may be a gear icon, depending on your browser's zoom level)\n",
    "\n",
    "  b. In the **Pipeline settings** section, you can:\n",
    "\n",
    "  - Modify the **Pipeline name**, if desired\n",
    "\n",
    "  - Change the **Run as** principle. To do this, select the pencil icon next to **Run as** to modify the option. You can only change this if there are other users in your workspace.\n",
    "\n",
    "  - You can optionally change the executor of the pipeline to a service principal. A service principal is an identity you create in Databricks for use with automated tools, jobs, and applications.  \n",
    "\n",
    "  - For more information, see the [What is a service principal?](https://docs.databricks.com/aws/en/admin/users-groups/service-principals#what-is-a-service-principal) documentation.\n",
    "\n",
    "    - In **Pipeline mode**, ensure **Triggered** is selected so the pipeline runs on a schedule.  \n",
    "      - Alternatively, you can choose **Continuous** mode to keep the pipeline running at all times.  \n",
    "      - For more details, see [Triggered vs. continuous pipeline mode](https://docs.databricks.com/aws/en/dlt/pipeline-mode).\n",
    "\n",
    "  c. In the **Code assets** section, you can change the **Root Folder** or **Source code** files, as needed.\n",
    "\n",
    "  d. The **Default location for data assets** section gives you the ability to change where tables, views, etc., will be created/refreshed, by default.\n",
    "\n",
    "  e. In the **Compute** section, confirm that **Serverless** compute is selected.\n",
    "\n",
    "  f. You can add libraries or a `requirements.txt` file in the **Pipeline environment** section.\n",
    "\n",
    "  g. We need to add a couple of configuration parameters to the pipeline that will be used by the pipeline's source code file. Complete the following:\n",
    "\n",
    "  h. Scroll down to the **Configuration** section, and click **Add configuration**.\n",
    "\n",
    "  - Set the **key** for the first parameter to \"catalog_name\"\n",
    "  - Set the **value** for this key to \"dbacademy\" (or a different one if you changed the default at the beginning of the lesson) \n",
    "  - Set the **key** for the first parameter to \"schema_name\"\n",
    "  - Set the **value** for this key to \"create_pipeline\" (or a different one if you changed the default at the beginning of the lesson) \n",
    "  - Click **Save**\n",
    "  - Close the settings by clicking the \"X\" in the upper-right corner\n",
    "\n",
    "  i. The **Tags**, **Usage**, and **Notifications** sections are not discussed in this course.\n",
    "\n",
    "  j. While we are here, we are going to setup this pipeline for the next lesson. Click **Edit advanced settings**.\n",
    "\n",
    "  - Expand **Advanced settings** at the bottom of the window.\n",
    "\n",
    "  - For **Channel**, you can select either **Current** or **Preview**:\n",
    "    - **Current** – Uses the latest stable Databricks Runtime version, recommended for production.\n",
    "    - **Preview** – Uses a more recent, potentially less stable Runtime version, ideal for testing upcoming features.\n",
    "    - View the [Lakeflow Spark Declarative Pipelines release notes and the release upgrade process](https://docs.databricks.com/aws/en/release-notes/dlt/) documentation for more information.\n",
    "\n",
    "  - We can also publish the pipeline's event log to a Delta table. We will see in the next lesson that a summary of the pipeline's events is available to us in the UI, but we can get more detailed information saved, if desired.\n",
    "\n",
    "    - Click **Save** to save the advanced settings.\n",
    "  k. Click the \"X\" in the upper-right corner to close the Pipeline settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc164b15-41d8-4b7e-b3a7-0d15f3591333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Schedule a Pipeline\n",
    "\n",
    "Once your pipeline is production-ready, you may want to schedule it to run either on a time interval or continuously. For this demonstration, we’ll configure a schedule, but not actually implement it.\n",
    "\n",
    "Complete the following steps to schedule the pipeline:\n",
    "\n",
    "a. Select the **Schedule** button in the upper-right corner (might be a small calendar icon depending on your browser's zoom level).\n",
    "\n",
    "b. Click **Add schedule**.\n",
    "\n",
    "When you schedule a pipeline, you are actually creating a single task Lakeflow Job that will run the pipeline according to the schedule you set.\n",
    "\n",
    "c. For the job name, leave it as is.\n",
    "\n",
    "d. Below **Job name**, select **Advanced**.\n",
    "\n",
    "e. In the **Schedule** section, configure the following:\n",
    "- Set the **Day**.\n",
    "- Set the time to **20:00** (8:00 PM).\n",
    "- Leave the **Timezone** as default.\n",
    "- Select **More options**, and under **Notifications**, add your email to receive alerts for:\n",
    "  - **Start**\n",
    "  - **Success**\n",
    "  - **Failure**\n",
    "\n",
    "f. Optionally, uncheck **Performance optimized** if you wish to save money, but increase startup time.\n",
    "\n",
    "g. Since we are not actually scheduling this pipeline, click **Cancel**.\n",
    "\n",
    "**NOTE:** You could also set the pipeline to run a few minutes after your current time to see it start through the scheduler."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Creating and Managing Spark Declarative Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}