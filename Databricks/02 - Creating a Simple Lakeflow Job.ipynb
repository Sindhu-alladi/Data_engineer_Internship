{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11779117-7dc8-4551-8e35-2ac5b48f20d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![databricks_academy_logo.png](../Includes/images/databricks_academy_logo.png \"databricks_academy_logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a2a79d8-b9c1-4a7a-8b15-9ab026ba552d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating a Simple Lakeflow Job\n",
    "\n",
    "Lakeflow Jobs provides a collection of tools that allow you to schedule and orchestrate all processing tasks on Databricks.\n",
    "\n",
    "**Objective:** Use Databricks Lakeflow Jobs to create a two task job. The job has been separated into two notebooks for demonstration purposes:\n",
    "- **Jobs - Task 1 - Setup - Bronze**\n",
    "- **Jobs - Task 2 - Silver - Gold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfebc4de-a8f5-4abb-8382-6381ae322046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Important: Select Environment 4\n",
    "The cells below may not work in other environments. To choose environment 4: \n",
    "1. Click the ![environment.png](../Includes/images/environment.png \"environment.png\") button on the right sidebar\n",
    "1. Open the **Environment version** dropdown\n",
    "1. Select **4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c191892-e416-4497-b0e4-cb3c01de1140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb83a7ec-0263-4e72-9667-73eb3c2e28c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Set python variables for catalog, schema, and volume names (change, if desired)\n",
    "catalog_name = \"dbacademy\"\n",
    "schema_name = \"create_job\"\n",
    "volume_name = \"myfiles\"\n",
    "####################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Create the catalog, schema, and volume if they don't exist already\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_name}\")\n",
    "####################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Creates a file called employees.csv in the specified catalog.schema.volume\n",
    "import pandas as pd\n",
    "data = [\n",
    "    [\"1111\", \"Kristi\", \"USA\", \"Manager\"],\n",
    "    [\"2222\", \"Sophia\", \"Greece\", \"Developer\"],\n",
    "    [\"3333\", \"Peter\", \"USA\", \"Developer\"],\n",
    "    [\"4444\", \"Zebi\", \"Pakistan\", \"Administrator\"]\n",
    "]\n",
    "columns = [\"ID\", \"Firstname\", \"Country\", \"Role\"] \n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "file_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/employees.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "################################################################################\n",
    "\n",
    "####################################################################################\n",
    "# Creates a file called employees2.csv in the specified catalog.schema.volume\n",
    "data = [\n",
    "    [5555, 'Alex','USA', 'Instructor'],\n",
    "    [6666, 'Sanjay','India', 'Instructor']\n",
    "]\n",
    "columns = [\"ID\",\"Firstname\", \"Country\", \"Role\"]\n",
    "\n",
    "## Create the DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "## Create the CSV file in the course Catalog.Schema.Volume\n",
    "df.to_csv(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/employees2.csv\", index=False)\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62616d77-47b1-42c0-b51c-350529c781f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1. Generate Lakeflow Job Configuration\n",
    "\n",
    "Configuring this lakeflow job will require parameters unique to a given user.\n",
    "\n",
    "Run the cell below to print out values you'll use to configure your lakeflow job in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73238874-734b-436d-ad56-9ac7b81c6cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name your LakeFlow Job: create_job_Example\n\nCatalog name: dbacademy\nSchema name: create_job\n\nNOTEBOOK PATHS FOR TASKS:\n\n  * Task 1 notebook path: \n   /Users/alladisindhu24@gmail.com/Data Engineering/M-04 -- Orchestration/Jobs - Task 1 - Setup - Bronze\n\n  * Task 2 notebook path: \n   /Users/alladisindhu24@gmail.com/Data Engineering/M-04 -- Orchestration/Jobs - Task 2 - Silver - Gold\n"
     ]
    }
   ],
   "source": [
    "path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n",
    "newpath = path.replace('02 - Creating a Simple Lakeflow Job','')\n",
    "task1path = newpath + 'Jobs - Task 1 - Setup - Bronze'\n",
    "task2path = newpath + 'Jobs - Task 2 - Silver - Gold'\n",
    "\n",
    "print(f'Name your LakeFlow Job: {schema_name}_Example\\n')\n",
    "print(f'Catalog name: {catalog_name}')\n",
    "print(f'Schema name: {schema_name}\\n')\n",
    "print(f'NOTEBOOK PATHS FOR TASKS:\\n')\n",
    "print(f'  * Task 1 notebook path: \\n   {task1path}\\n')\n",
    "print(f'  * Task 2 notebook path: \\n   {task2path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6f51fc-8315-4636-8301-c8ace6c44615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2. Configure Job with a Notebook Task\n",
    "\n",
    "When using the Jobs UI to orchestrate a job with multiple tasks, you'll always begin by creating a job with a single task, and can add more if required.\n",
    "\n",
    "Complete the following to create a Lakeflow job with two tasks using the notebooks from above (**DEWD00 - 04A-Task 1 - Setup - Bronze** and **DEWD00 - 04B-Task 2 - Silver - Gold**):\n",
    "\n",
    "1. Right-click the **Jobs & Pipelines** button on the sidebar, and open the link in a new tab. This way, you can refer to these instructions, as needed.\n",
    "\n",
    "2. Confirm you are in the **Jobs & Pipelines** tab.\n",
    "\n",
    "3. On the right side, select **Create -> Job**.\n",
    "\n",
    "4. In the top-left of the screen, enter the **Job Name** provided above to add a name for the Lakeflow job.\n",
    "\n",
    "5. Configure Job parameters by clicking, **Edit parameters** on the right-hand side of the jobs UI.\n",
    "- Set the **key** for the first parameter to \"catalog_name\"\n",
    "- Set the **value** for this key to the **Catalog name** in the output of the previous cell (default is \"dbacademy\")\n",
    "- Set the **key** for the first parameter to \"schema_name\"\n",
    "- Set the **value** for this key to the **Schema name** in the output of the previous cell (default is \"create_job\")\n",
    "- Click **Save**\n",
    "\n",
    "6. Under **Add your first task**, select **Notebook**. If **Notebook** is not listed, click **+ Add another task type** and choose **Notebook** from the options.\n",
    "\n",
    "7. Follow the instructions below to set up your tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf79b5b-8fc3-4558-8f05-8ec9b1bb4a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create Task 1\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| Task name | Enter **Setup-Bronze** |\n",
    "| Type | Ensure **Notebook** is selected. Note in the dropdown list the many different types of tasks that can be scheduled |\n",
    "| Source | Ensure **Workspace** is selected |\n",
    "| Path | Use the navigator to specify the **Jobs - Task 1 - Setup - Bronze** notebook. Use the path from above to help find the notebook. |\n",
    "| Compute     | Select **Serverless** from the dropdown menu.<br>\n",
    "| Environment and Libraries| Ensure **Default** is selected |\n",
    "| Create | Select the **Create task** button to create the task |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a6f758-3a30-475a-a27a-3e0ec27d26ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Task 2\n",
    "| Setting | Instructions |\n",
    "|--|--|\n",
    "| New task | Select **Add task** within your job. Then select **Notebook**|\n",
    "| Task name | Enter **Silver-Gold** |\n",
    "| Type | Choose **Notebook**. Note in the dropdown list the many different types of lakeflow jobs that can be scheduled |\n",
    "| Source | Choose **Workspace** |\n",
    "| Path | Use the navigator to specify the **Jobs - Task 2 - Silver - Gold** notebook. Use the path from above to help find the notebook. |\n",
    "| Compute     | Select **Serverless** from the dropdown menu.|\n",
    "| Depends on | Select **Setup-Bronze** |\n",
    "| Run if dependencies | Select **All succeeded** |\n",
    "| Environment and Libraries| Ensure **Default** is selected |\n",
    "| Create | Select the **Create task** button to create the task |\n",
    "\n",
    "##### For better performance, please enable Performance Optimized Mode in Job Details. Otherwise, it might take 6 to 8 minutes to initiate execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f56fc9c2-4752-4d67-ae35-4e94fa2b68fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## 3. Explore Scheduling Options\n",
    "Complete the following steps to explore the scheduling options:\n",
    "\n",
    "1. On the right hand side of the Jobs UI, locate the **Schedules & Triggers** section.\n",
    "\n",
    "2. Select the **Add trigger** button to explore scheduling options.\n",
    "\n",
    "3. Changing the **Trigger type** from **None (Manual)** to **Scheduled** will bring up a scheduling UI.\n",
    "\n",
    "   - This UI provides extensive options for setting up chronological scheduling of your LakeFlow Jobs. Settings configured with the UI can also be output in cron syntax, which can be edited as needed.\n",
    "   \n",
    "4. Select **Cancel** to return to Job details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65056a39-a356-4e2b-8099-f43101c4f2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Run Job\n",
    "Select **Run now** above  **Job details** to execute the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5afda061-2022-4a99-9f71-3b231aec3f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Review Job Run\n",
    "\n",
    "To review the job run:\n",
    "\n",
    "1. On the Job details page, select the **Runs** tab in the top-left of the screen (you should currently be on the **Tasks** tab)\n",
    "\n",
    "1. Open the output details by clicking on the timestamp field under the **Start time** column\n",
    "\n",
    "    - If **the job is still running**, you will see the active state of the notebook with a **Status** of **`Pending`** or **`Running`** in the right side panel.\n",
    "\n",
    "    - If **the job has completed**, you will see the full execution of the notebook with a **Status** of **`Succeeded`** or **`Failed`** in the right side panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ece7d5-569f-41e9-b213-bdd5cb4b912d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. DROP the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabb5ebd-89aa-4995-b6ba-71e3e4119608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP SCHEMA IF EXISTS {schema_name} CASCADE;\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Creating a Simple Lakeflow Job",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}